---
title: "First_fits"
author: "Jostein Aasteboel, Aanes and 10920007"
date: "2023-09-27"
output: html_document
---

## Load libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Load libraries
library(INLA)
library(tidyverse)
library(spData)
library(sf)
library(spdep)
library(ggplot2)
library(ggspatial)
require(mgcv)
library(MASS)
library(fastmatrix)
library(rwc)
library(gridExtra)
library(ggpubr)
library(roahd)

#Load other files
source("functions_plotting_etc.R")
```

## Load

```{r}
#Load structure matrices
struct_RW1 = readMM(file='struct_RW1.txt')
struct_RW2 = readMM(file = 'struct_RW2.txt')
ICAR_structure = readMM(file = 'ICAR_struct.txt')

#Load data
ohio_df <- read.csv("ohio_df.csv")

#read the shapefile of ohio
ohio_map  <- read_sf('./fe_2007_39_county')[ ,c("geometry", "NAME")]
ohio_map <- ohio_map[order(ohio_map$NAME), ]

#Calculate adjacencies (if needed)
nb <- spdep::poly2nb(ohio_map, queen = FALSE)

#Get number of counties (n) and number of years (T)
n <- length(unique(ohio_df$county))   # Number of areas
T <- length(unique(ohio_df$year))     # Number of time points

#Used for heat map plotting
#bins_hardcoded = c(0, 0.15, 0.3, 0.5, 0.65, 0.8, 1, 1.25)
```


## Explorative plot of data

```{r}
#Dont know if necessary to merge now at all?
to_plot_potentially <- merge(ohio_map, ohio_df,
                  by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)
```


### Heatmaps
```{r}
#From the heatmaps it seems that the rate is increasing over time 
plot_all_years(to_plot_potentially, ncol = 7, nrow = 6,
               widths = c(2, 0.05, 2, 0.05, 2, 0.05, 2))
```

### Depth measures for the time series of each county
```{r}
#Depth measures over time-series for each county???
grid = unique(ohio_df$year)
temp_ = ohio_df[ohio_df$county_name == unique(ohio_df$county_name)[1], ]
temp_data = temp_$rate
for(county in unique(ohio_df$county_name)[2:length(unique(ohio_df$county_name))]){
  temp_ = ohio_df[ohio_df$county_name == county, ]
  temp_data = rbind(temp_data, temp_$rate)
  #temp_fd = fData(grid, temp_$deaths)
}
fdata <- fData(grid, temp_data)

band_depth = BD(fdata)
modified_band_depth = MBD(fdata)
median_curve = median_fData(fdata, type = "MBD")


#Weak evidence of trend, maybe RW2 performs better
plot(fdata)
lines(grid, median_curve$values, lwd = 3)

#indicates a trend, there is also some weird shapes
```



## Hyperparameters and corresponding priors
```{r}
#Temporal
temporal_hyper = list(prec.unstruct = list(prior = 'pc.prec', 
                                           param = c(1, 0.01)), #Magic numbers
                      prec.spatial = list(prior = 'pc.prec', 
                                           param = c(1, 0.01)) #Magic numbers
) 

#Spatial
spatial_hyper = list(prec.unstruct = list(prior = 'pc.prec', 
                                           param = c(1, 0.01)), #Magic numbers
                      prec.spatial = list(prior = 'pc.prec', 
                                           param = c(1, 0.01)) #Magic numbers
)

```


## Base linear predictor w.o. interaction terms
```{r}

#Make the base formula (is the overall mean specified correctly here???)
basic_linear_formula <- deaths ~ 1 + f(year, 
                                   model = 'bym',
                                   scale.model = T, 
                                   constr = T, 
                                   rankdef = 1,
                                   graph = struct_RW1,
                                   hyper = temporal_hyper
                                   ) + 
                                  f(county, 
                                    model = 'bym',
                                    scale.model = T,
                                    constr = T,
                                    rankdef = 1,
                                    graph = ICAR_structure,
                                    hyper = spatial_hyper
                                  )

#Measure time to do inference
ptm <- Sys.time()
basic_model_fit <- inla(basic_linear_formula,
                 data = ohio_df,
                 family = "poisson",
                 E = pop_at_risk, #Is it supposed to be expected or population at risk???
                 control.compute = list(config = TRUE, # needed if you want to see constraints later
                                        cpo = T,
                                        waic = T), #Needed for model selection
                 verbose = F #Not T unless want to see a lot of stuff
                 ) 
time = Sys.time()-ptm


print(c("Number of constraints (should be 2): ",toString(basic_model_fit$misc$configs$constr$nc)))
print(c("mean CPO:", toString(round(mean(basic_model_fit$cpo$cpo), digits = 4))))
print(c("CPO failiure, should have 1 unique value",
        toString(length(unique(basic_model_fit$cpo$failure))))) #-> soinla.cpo(res)
print(c("And that value is (should be 0): ", toString(basic_model_fit$cpo$failure[1])))
print(c("CPU: ", toString(summary(basic_model_fit)$cpu.used)))
print(c("Time to compute", toString(time)))
```

### Inference on basic model

#### Check the random effects +++

```{r}
#plot(basic_model_fit)
p <- plot(basic_model_fit$summary.random$year$ID[1:T], 
     basic_model_fit$summary.random$year$mean[1:T], 
     type = "l", lty = 1, xlab = "Year: t", ylab = "random effect alpha_t") + 
  lines(basic_model_fit$summary.random$year$ID[1:T],
        basic_model_fit$summary.random$year$'0.025quant'[1:T],
        type = "l", lty = 2, col = "red") + 
  lines(basic_model_fit$summary.random$year$ID[1:T],
        basic_model_fit$summary.random$year$'0.975quant'[1:T],
        type = "l", lty = 2, col = "green")

p
```


#### Heatmap of fitted values
```{r}
#Extract predictions
#Make dataframe with years and county marked
fitted_values <- data.frame(basic_model_fit$summary.fitted.values$mean)

#This is not actually rate here tho. It is fitted values which is ???
colnames(fitted_values) <- "rate"

fitted_values$year <- ohio_df$year
fitted_values$county <- ohio_df$county
fitted_values$county_name <- ohio_df$county_name
fitted_values$pop_at_risk <- ohio_df$pop_at_risk

#Merge with geometry
fitted_values <- merge(ohio_map, fitted_values,
                   by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)


plot_all_years(fitted_values, ncol = 7, nrow = 6,
               widths = c(2, 0.05, 2, 0.05, 2, 0.05, 2))
```


```{r}
#Get difference and plot
fitted_values$rate <- fitted_values$rate - ohio_df$rate

#From plots its clear that predictions gets worse and worse
#But for certain regions only
plot_all_years(fitted_values, bins_hardcoded,
               ncol = 7, nrow = 6,
               widths = c(2, 0.05, 2, 0.05, 2, 0.05, 2))
```

## Model w. type I interaction fitted

```{r}
# Type I Interaction:
interaction_hypers = list(prec = list(param = c(1, 0.01)))

typeI_formula <- update(basic_linear_formula, 
                        ~. + f(space_time_unstructured,
                               model="iid", #Has to be iid, whole point
                               hyper = interaction_hypers ))
      




ptm <- Sys.time()
typeI_fit <- inla(typeI_formula,
                  data = ohio_df,
                  family = 'poisson',
                  E = pop_at_risk,
                  control.compute = list(config = TRUE, # needed if you want to see constraints later
                                         cpo = TRUE,    #Needed for model choice
                                         waic = TRUE), 
                  verbose = F #Not T unless want to see a lot of stuff
                  )
time = Sys.time() - ptm
print(c("Number of constraints (should be 2): ",toString(typeI_fit$misc$configs$constr$nc)))
print(c("mean CPO:", toString(round(mean(typeI_fit$cpo$cpo), digits = 4))))
print(c("CPO failiure, should have 1 unique value",
        toString(length(unique(typeI_fit$cpo$failure))))) #-> soinla.cpo(res)
print(c("And that value is (should be 0): ", toString(typeI_fit$cpo$failure[1])))
print(c("CPU: ", toString(summary(typeI_fit)$cpu.used)))
print(c("Time: ", time))
```

### Heatmap for each year of linear predictor w. typeI interactions
```{r}
#Extract predictions
#Make dataframe with years and county marked
fitted_values <- data.frame(model_w_type_I_fit$summary.fitted.values$mean)

#This is not actually rate here tho. It is fitted number of deaths
colnames(fitted_values) <- "rate"

fitted_values$year <- ohio_df$year
fitted_values$county <- ohio_df$county
fitted_values$county_name <- ohio_df$county_name
fitted_values$pop_at_risk <- ohio_df$pop_at_risk

#Merge with geometry
fitted_values <- merge(ohio_map, fitted_values,
                   by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)


plot_all_years(fitted_values, bins_hardcoded,
               ncol = 7, nrow = 6,
               widths = c(2, 0.05, 2, 0.05, 2, 0.05, 2))

```
```{r}
#Get difference and plot
fitted_values$rate <- fitted_values$rate - ohio_df$rate

#The fit gets progressively worse with time
plot_all_years(fitted_values, bins_hardcoded,
               ncol = 7, nrow = 6,
               widths = c(2, 0.05, 2, 0.05, 2, 0.05, 2))
```

## Model w. type II interaction

```{r}
# Type II: RW-time x iid-space
# creating constraint matrix needed for Type II Interaction

#sum-to-zero constraint be wilding (Is it the right way??? Does it actually do what it is supposed to???)
# - n rows as this interaction makes it so there is a RW for each county independent of each other
# - n * T columns as there are a total of n * T interaction terms (delta_(it))
# - sets A[i, which((0:(n * T - 1))%%n == i - 1)] = 1, as for county i, the interaction (delta_(it))
#     is only dependent on (delta_(i,t+-1)). Therefore the sum-to-zero is over these 88 RWs
# - Constraints: The RW1 in each area needs to sum to 0. Hence in constr.st e is a zero vector
A <- matrix(0, nrow = n, ncol = n * T)
for (i in 1:n) {
  #Should it be 0:(n*T - 1)%%(n) == i - 1, used to be 1:(n*T)%%n == i - 1
  A[i, which((0:(n * T - 1))%%n == i - 1)] <- 1
}



constr.st <- list(A = A, e = rep(0, dim(A)[1]))

#scaled_RW_prec <- inla.scale.model(struct_RW1,
#                                   list(A = matrix(1, 1, dim(struct_RW1)[1]),
#                                                    e = 0))

# Kronecker product between RW1 and IID space term
# order matters here! In our data set, the time ordering take precedent over the space ordering
# so we must have the RW on the left and space on the right
R <- struct_RW1 %x% diag(n)

interaction_param = c(1, 0.01)

typeII_formula <- update(basic_linear_formula,
                         ~. + f(space_time_unstructured, 
                                model = "generic0", 
                                Cmatrix = R, 
                                extraconstr = constr.st, 
                                rankdef = n, 
                                param = interaction_param))



ptm <- Sys.time()
modII <- inla(typeII_formula,
              data = ohio_df,
              family = "poisson",
              E = pop_at_risk,
              control.compute = list(config = TRUE,
                                     cpo = TRUE,
                                     waic = TRUE))
time = Sys.time() - ptm

print(c("Number of constraints (should be 2 + 88 = 90): ",toString(modII$misc$configs$constr$nc)))
print(c("mean CPO:", toString(round(mean(modII$cpo$cpo), digits = 4))))
print(c("CPO failiure, should have 1 unique value",
        toString(length(unique(modII$cpo$failure))))) #-> soinla.cpo(res)
print(c("And that value is (should be 0): ", toString(modII$cpo$failure[1])))
print(c("CPU: ", toString(summary(modII)$cpu.used)))
print(c("Time: ", time))
```

### Heatmaps etc
```{r}
#Extract predictions
#Make dataframe with years and county marked
fitted_values <- data.frame(modII$summary.fitted.values$mean)

#This is not actually rate here tho. It is fitted number of deaths
colnames(fitted_values) <- "rate"

fitted_values$year <- ohio_df$year
fitted_values$county <- ohio_df$county
fitted_values$county_name <- ohio_df$county_name
fitted_values$pop_at_risk <- ohio_df$pop_at_risk

#Merge with geometry
fitted_values <- merge(ohio_map, fitted_values,
                   by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)

plot_all_years(fitted_values, bins_hardcoded)
```

```{r}
#Get difference and plot
fitted_values$rate <- fitted_values$rate - ohio_df$rate

#Gets progressively worse with time
plot_all_years(fitted_values, bins_hardcoded)
```

## Model w. type III interaction


```{r}

#Type III interaction

#- Constraints: The ICAR at each time needs to sum to 0

### creating constraint needed for Type III Interaction
A <- matrix(0, nrow = T, ncol = n * T)
for (i in 1:T) {
  # The ICAR at each time point needs to sum to 0
  A[i, ((i - 1) * n + 1):(i * n)] <- 1
}

### defining Kronecker product for the Type III Interaction
# get scaled ICAR
#scaled_ICAR_prec <- INLA::inla.scale.model(ICAR_structure, 
#                                           constr = list(A = matrix(1,
#                                                                    1,
#                                                                    dim(ICAR_structure)[1]),
#                                                         e = 0))

# Kronecker product between IID x ICAR
R <- diag(T) %x% ICAR_structure 

# specify constraints in INLA-ready format
constr.st <- list(A = A, e = rep(0, dim(A)[1]))

interactions_param = c(1, 0.01)

#Model w. type III interaction
typeIII_formula <- update(basic_linear_formula, 
                          ~. + f(space_time_unstructured, 
                                model = "generic0", 
                                Cmatrix = R, 
                                extraconstr = constr.st, 
                                rankdef = T, 
                                param = interaction_param))


ptm <- Sys.time()
modIII <- inla(typeIII_formula,
              data = ohio_df,
              family = "poisson",
              E = pop_at_risk,
              control.compute = list(config = TRUE, 
                                     cpo = TRUE,
                                     waic = TRUE))
time = Sys.time() - ptm


print(c("Number of constraints (should be 2 + 21 = 23): ",toString(modIII$misc$configs$constr$nc)))
print(c("mean CPO:", toString(round(mean(modIII$cpo$cpo), digits = 4))))
print(c("CPO failiure, should have 1 unique value",
        toString(length(unique(modIII$cpo$failure))))) #-> soinla.cpo(res)
print(c("And that value is (should be 0): ", toString(modIII$cpo$failure[1])))
print(c("CPU: ", toString(summary(modIII)$cpu.used)))
print(c("Time: ", time))

```
### Heatmaps and stuff

```{r}
#Extract predictions
#Make dataframe with years and county marked
fitted_values <- data.frame(modIII$summary.fitted.values$mean)

#This is not actually rate here tho. It is fitted number of deaths
colnames(fitted_values) <- "rate"

fitted_values$year <- ohio_df$year
fitted_values$county <- ohio_df$county
fitted_values$county_name <- ohio_df$county_name
fitted_values$pop_at_risk <- ohio_df$pop_at_risk

#Merge with geometry
fitted_values <- merge(ohio_map, fitted_values,
                   by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)

plot_all_years(fitted_values, bins_hardcoded)

```
```{r}
#Get difference and plot
fitted_values$rate <- fitted_values$rate - ohio_df$rate

#Gets worse with time
plot_all_years(fitted_values, bins_hardcoded)
```

## Model w. type IV interaction

```{r}

# Type IV Interaction ----------------------------------------------------------

# Specifying constraints needed for Type IV Interaction
time_constr <- matrix(0, n, n * T)
for (i in 1:n) {
  time_constr[i, which((0:(n * T - 1))%%n == i - 1)] <- 1
}
space_constr <- matrix(0, T-1, n * T)
# theoretically it's enough to only go through N-1 here
# note that we could have instead done 1:(T-1) in the first loop and 1:n in the second
for (i in 1:(T-1)) { 
  space_constr[i, ((i - 1) * n + 1):(i * n)] <- 1
}


tmp <- rbind(time_constr, space_constr) 
constr.st <- list(A = tmp, e = rep(0, dim(tmp)[1]))

# Kronecker product between ICAR x RW1
R <- struct_RW1 %x% ICAR_structure

formulaIV <- update(basic_linear_formula,
                    ~. + f(space_time_unstructured, 
                           model = "generic0",
                           Cmatrix = R,
                           extraconstr = constr.st,
                           rankdef = n + T - 1, 
                           param = c(1, 0.01)))

#  Time difference of ~ 20 minutes on my personal machine

ptm <- Sys.time()
modIV <- inla(formulaIV,
               data = ohio_df,
               family = "poisson",
               E = pop_at_risk,
               control.compute = list(config = TRUE,
                                      cpo = TRUE,
                                      waic = TRUE))

time = Sys.time() - ptm




# Check the number of constraints in our model
# We should have:
# - 1 sum-to-zero constraint for the BYM2 in space
# - 1 sum-to-zero constraint for the BYM2 in time
# - N + S - 1 constraints for the Type IV Interaction
# - N + S + 1 constraints total

#print(c("Number of constraints (should be 1 + n + T = 110): ",toString(modIV$misc$configs$constr$nc)))
#print(c("mean CPO:", toString(round(mean(modIV$cpo$cpo), digits = 4))))
#print(c("CPO failiure, should have 1 unique value",
#        toString(length(unique(modIV$cpo$failure))))) #-> soinla.cpo(res)
#print(c("And that value is (should be 0): ", toString(modIV$cpo$failure[1])))
#print(c("CPU: ", toString(summary(modIV)$cpu.used)))
#print(c("Time: ", time))

```

### Heatmaps and stuff

```{r}
#Extract predictions
#Make dataframe with years and county marked
fitted_values <- data.frame(modIV$summary.fitted.values$mean)

#This is not actually rate here tho. It is fitted number of deaths
colnames(fitted_values) <- "rate"

fitted_values$year <- ohio_df$year
fitted_values$county <- ohio_df$county
fitted_values$county_name <- ohio_df$county_name
fitted_values$pop_at_risk <- ohio_df$pop_at_risk

#Merge with geometry
fitted_values <- merge(ohio_map, fitted_values,
                   by.x = c("NAME"), by.y = c("county_name"),
                   all = T, suffixes = T)

plot_all_years(fitted_values, bins_hardcoded)
```

```{r}
#Get difference and plot
fitted_values$rate <- fitted_values$rate - ohio_df$rate

#Gets worse with time
plot_all_years(fitted_values, bins_hardcoded)
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```



